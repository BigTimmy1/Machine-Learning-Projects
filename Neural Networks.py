# -*- coding: utf-8 -*-
"""CoAI3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DDqhhVgO712AsjxbO9yqxTvc9sG_drl
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Load data
data = pd.read_csv('nba_rookie_data.csv')
data2 = data.drop(['Name'], axis=1)

data2.dropna(inplace=True)  # Remove rows with missing values

# Display data overview
print("Data Summary:")
data2.describe(include='all')

# Split features and target
X = data2.drop('TARGET_5Yrs', axis=1)
y = data2['TARGET_5Yrs']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Standardize features for models that require scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'LogisticRegression': LogisticRegression(solver='liblinear', max_iter=1000),
    'GaussianNaiveBayes': GaussianNB(),
    'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)
}

# Train and evaluate models
def evaluate_model(model, X_test_data, y_true, model_name):
    predictions = model.predict(X_test_data)
    accuracy = accuracy_score(y_true, predictions)
    report = classification_report(y_true, predictions, output_dict=True)
    # Collect relevant metrics
    class_metrics = report['weighted avg']
    metrics_dict = {
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': class_metrics['precision'],
        'Recall': class_metrics['recall'],
        'F1-Score': class_metrics['f1-score']
    }
    return metrics_dict

results_before_tuning = []
for model_name, model in models.items():
    if model_name == 'NeuralNetwork':
        model.fit(X_train_scaled, y_train)  # Use scaled data for neural network
    else:
        model.fit(X_train, y_train)  # Non-scaled for others
    metrics = evaluate_model(model, X_test_scaled if model_name == 'NeuralNetwork' else X_test, y_test, model_name)
    results_before_tuning.append(metrics)

# Convert results to DataFrame
results_before_tuning_df = pd.DataFrame(results_before_tuning)

results_before_tuning_df

# Define parameter grids for hyperparameter tuning
param_grids = {
    'LogisticRegression': {'C': [0.001, 0.01, 0.1, 1, 10, 100]},
    'GaussianNaiveBayes': {},  # No hyperparameters for Naive Bayes
    'NeuralNetwork': {'hidden_layer_sizes': [(5,), (10,), (15,)], 'alpha': [0.0001, 0.001, 0.01]}
}

# Perform hyperparameter tuning
def hyperparameter_tuning(model, param_grid, X_train_data, y_train_data):
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train_data, y_train_data)
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    print(f"Best parameters for {model.__class__.__name__}: {best_params}")
    return best_model

results_after_tuning = []
for model_name, model in models.items():
    param_grid = param_grids[model_name]
    best_model = hyperparameter_tuning(model, param_grid, X_train_scaled, y_train)
    metrics = evaluate_model(best_model, X_test_scaled, y_test, model_name)
    results_after_tuning.append(metrics)

# Convert post-tuning results to DataFrame
results_after_tuning_df = pd.DataFrame(results_after_tuning)

# Visualization of Before and After Tuning Results
fig, axes = plt.subplots(2, 2, figsize=(12, 12))
fig.suptitle("Model Performance Before and After Hyperparameter Tuning", fontsize=16)

# Combined bar plot function
def plot_metric(metric, ax, df1, df2):
    width = 0.35
    x = np.arange(len(models))
    ax.bar(x - width/2, df1[metric], width, label="Before Tuning", color='skyblue')
    ax.bar(x + width/2, df2[metric], width, label="After Tuning", color='salmon')
    ax.set_title(f'{metric}')
    ax.set_xticks(x)
    ax.set_xticklabels(df1['Model'])
    ax.legend()

# Plot each metric in separate subplot
plot_metric('Accuracy', axes[0, 0], results_before_tuning_df, results_after_tuning_df)
plot_metric('Precision', axes[0, 1], results_before_tuning_df, results_after_tuning_df)
plot_metric('Recall', axes[1, 0], results_before_tuning_df, results_after_tuning_df)
plot_metric('F1-Score', axes[1, 1], results_before_tuning_df, results_after_tuning_df)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Display final results
print("\nPerformance Metrics Before Tuning:")
print(results_before_tuning_df)
print("\nPerformance Metrics After Tuning:")
print(results_after_tuning_df)